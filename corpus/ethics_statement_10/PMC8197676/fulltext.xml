<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="research-article"><?properties open_access?><?OLF?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><?covid-19-tdm?><front><journal-meta><?QA-only?><journal-id journal-id-type="iso-abbrev">AI Ethics</journal-id><journal-title-group><journal-title>AI and Ethics</journal-title></journal-title-group><issn pub-type="ppub">2730-5953</issn><issn pub-type="epub">2730-5961</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">8197676</article-id><article-id pub-id-type="publisher-id">69</article-id><article-id pub-id-type="doi">10.1007/s43681-021-00069-w</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Research</subject></subj-group></article-categories><title-group><article-title>Ethical funding for trustworthy AI: proposals to address the responsibilities of funders to ensure that projects adhere to trustworthy AI practice</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8483-121X</contrib-id><name><surname>Gardner</surname><given-names>Allison</given-names></name><address><email>a.gardner@keele.ac.uk</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Smith</surname><given-names>Adam Leon</given-names></name><address><email>adam@wearedragonfly.co</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Steventon</surname><given-names>Adam</given-names></name><address><email>adam.steventon@health.org.uk</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Coughlan</surname><given-names>Ellen</given-names></name><address><email>ellen.coughlan@health.org.uk</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Oldfield</surname><given-names>Marie</given-names></name><address><email>Marie@oldfieldconsultancy.co.uk</email></address><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.9757.c</institution-id><institution-id institution-id-type="ISNI">0000 0004 0415 6205</institution-id><institution>School of Computing and Mathematics, </institution><institution>Keele University, </institution></institution-wrap>Newcastle-under-Lyme, ST5 5BG UK </aff><aff id="Aff2"><label>2</label>Dragonfly, London, UK </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.453604.0</institution-id><institution-id institution-id-type="ISNI">0000 0004 1756 7003</institution-id><institution>Health Foundation, </institution></institution-wrap>London, UK </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.499502.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2364 1394</institution-id><institution>Artificial Intelligence Group, </institution><institution>Royal Statistical Society, </institution></institution-wrap>London, UK </aff></contrib-group><pub-date pub-type="epub"><day>13</day><month>6</month><year>2021</year></pub-date><pub-date pub-type="pmc-release"><day>13</day><month>6</month><year>2021</year></pub-date><fpage>1</fpage><lpage>15</lpage><history><date date-type="received"><day>22</day><month>1</month><year>2021</year></date><date date-type="accepted"><day>2</day><month>6</month><year>2021</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2021</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">AI systems that demonstrate significant bias or lower than claimed accuracy, and resulting in individual and societal harms, continue to be reported. Such reports beg the question as to why such systems continue to be funded, developed and deployed despite the many published ethical AI principles. This paper focusses on the funding processes for AI research grants which we have identified as a gap in the current range of ethical AI solutions such as AI procurement guidelines, AI impact assessments and AI audit frameworks. We highlight the responsibilities of funding bodies to ensure investment is channelled towards trustworthy and safe AI systems and provides case studies as to how other ethical funding principles are managed. We offer a first sight of two proposals for funding bodies to consider regarding procedures they can employ. The first proposal is for the inclusion of a Trustworthy AI Statement&#x02019; section in the grant application form and offers an example of the associated guidance. The second proposal outlines the wider management requirements of a funding body for the ethical review and monitoring of funded projects to ensure adherence to the proposed ethical strategies in the applicants Trustworthy AI Statement. The anticipated outcome for such proposals being employed would be to create a &#x02018;stop and think&#x02019; section during the project planning and application procedure requiring applicants to implement the methods for the ethically aligned design of AI. In essence it asks funders to send the message &#x0201c;if you want the money, then build trustworthy AI!&#x0201d;.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Artificial intelligence</kwd><kwd>Trustworthy</kwd><kwd>Ethics</kwd><kwd>Funding</kwd><kwd>Framework</kwd></kwd-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Trustworthy AI<xref ref-type="fn" rid="Fn1">1</xref> has been a focus in the data science and AI field for several years. It has increased significantly in prominence and urgency with recent controversies involving public sector systems [<xref ref-type="bibr" rid="CR1">1</xref>] and influencing elections [<xref ref-type="bibr" rid="CR2">2</xref>].</p><p id="Par4">In the UK, August 2020 was a pivotal month in light of a number of legal cases and decisions challenging the use of some AI and machine learning systems. Examples include the judgement of the UK government visa streaming algorithm in August 2020 and its resultant suspension [<xref ref-type="bibr" rid="CR3">3</xref>]. This landmark legal challenge highlighted the human rights and equalities that can be caused by some AI systems [<xref ref-type="bibr" rid="CR4">4</xref>]. Similarly, the ground-breaking case challenging the facial recognition software trialled by South Wales police was upheld on appeal in the first legal case of its kind [<xref ref-type="bibr" rid="CR5">5</xref>]. Also, in August 2020, we saw the public uproar and legal challenge caused by the algorithm employed to predict grades for students. This clearly indicates that public awareness and impetus to hold AI systems to account is increasing. Around the same time other reports announced the withdrawal of child welfare algorithms by several councils [<xref ref-type="bibr" rid="CR6">6</xref>] and the suspension of the Most Serious Violence predictive system, part of the &#x000a3;10 million Home Office funded National Data Analytics Solution, by West Midlands Police on the advice of its Ethics Committee [<xref ref-type="bibr" rid="CR7">7</xref>]. Additionally, many cases have been heard globally and have been upheld [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>]. This clearly indicates that public awareness and impetus to hold AI systems to account is increasing.</p><p id="Par5">Despite a general belief that AI systems are more objective and accurate than humans, many algorithms are often not as accurate as claimed, or any more accurate than non-AI systems [<xref ref-type="bibr" rid="CR10">10</xref>]. They can also be significantly biased leading to discriminatory outcomes, as the cases mentioned above illustrate. Indeed, the language used to describe and sell AI often perpetuates a misleading view of AI quality, a significant concern for systems deemed as &#x02018;high-risk&#x02019;. High-risk AI applications are those that can potentially result in material harm to an individual or the environment if not correctly deployed e.g. diagnostic AI [<xref ref-type="bibr" rid="CR11">11</xref>], sentencing [<xref ref-type="bibr" rid="CR12">12</xref>], recruitment [<xref ref-type="bibr" rid="CR13">13</xref>], loan approval [<xref ref-type="bibr" rid="CR14">14</xref>] or chatbots designed to address mental health including addressing suicide [<xref ref-type="bibr" rid="CR15">15</xref>]. These high-risk AI applications are particularly vulnerable to harms caused by untrustworthy AI. The risks to human rights and indeed life, in the case of medical use of AI [<xref ref-type="bibr" rid="CR16">16</xref>], increases the urgency to find meaningful mechanisms to change the way we invest in, develop and use AI solutions. If we do not, it is likely that we will continue to see harm occurring and see further litigation.</p><p id="Par6">To illustrate this further, it is worth noting examples of diagnostic and predictive algorithms in the health care setting.</p><p id="Par7">A well-known example is that of a health care risk-prediction algorithm used on more than 200 million US citizens to identify patients who would benefit from &#x0201c;high-risk care management program&#x0201d; [<xref ref-type="bibr" rid="CR17">17</xref>]. The aim of this management program is to provide chronically ill people specially trained nursing staff and extra primary-care visits. However, this algorithm demonstrated significant racial bias in that Black patients assigned the same level of risk by the algorithm are sicker than a White patient. The authors report that this bias reduced the number of Black patients identified for extra care by more than a half. The reason this bias existed is because one of the features (or rules) the algorithm uses is that of health costs. This feature was used as a proxy for determining health need. However, due to unequal access to healthcare less money is spent on Black patients than equally sick White patients. In effect the algorithm was predicting health care costs not level of illness. The authors also showed that once this issue was noted and the algorithm modified to remove health costs the racial bias was eliminated.</p><p id="Par8">The Covid-19 pandemic has also resulted in many technological solutions and a very recent case has been highlighted with the QCovid living risk prediction algorithm used in the UK. This algorithm estimates risk of hospital admission and mortality from coronavirus 19 in adults [<xref ref-type="bibr" rid="CR18">18</xref>]. The algorithm used features such as age, ethnicity, deprivation, BMI and a range of comorbidities and had a sensitivity (number of correctly identified positives) of 75.7%. Commendably the authors did test their algorithm performance on men and women (still a rare occurrence) and found little difference. QCovid was used by the Joint Committee on Vaccination and Immunisation (JCVI) to determine who should have priority for the vaccine roll out. However, it has recently been reported [<xref ref-type="bibr" rid="CR19">19</xref>] by a JCVI committee member that the algorithm was likely to underestimate the risk to vulnerable people suffering from rare disease, particularly younger patients. The result being this group of patients, who are at high risk, were not prioritised for the vaccine. The committee member also pointed out that the datasets used to train the model may have other significant omissions due to some groups effectively shielding and not being exposed to the virus. Although this bias has not been verified it does reveal the importance of transparency and understanding of how such algorithms work if they are to be used to drive healthcare policy.<xref ref-type="fn" rid="Fn2">2</xref></p><p id="Par10">Indeed, it is not the first algorithm being used to determine vaccine policy to be questioned in this way. Stanford Medicine officials used an algorithm to determine which of their staff should be prioritized for the vaccine. However, it prioritized high-ranking doctors, with little patient-facing contact over residents involved in direct care of Covid-19 patients. The error here was that the junior doctors did not have an assigned location and were young. This resulted in demonstrations by the doctors and public coverage. The leadership stated that they used the algorithm to ensure equity but issued an apology and changed their vaccination policy.</p><p id="Par11">In response to these issues we have seen a significant number of High Level AI Principles (outlined later), frameworks [<xref ref-type="bibr" rid="CR20">20</xref>] and standards being developed for example IEEE P7010 Transparency of Autonomous Systems [<xref ref-type="bibr" rid="CR21">21</xref>] from the IEEEE P7000 series [<xref ref-type="bibr" rid="CR22">22</xref>] and ISO/IEC JTC 1/SC 42 Artificial Intelligence [<xref ref-type="bibr" rid="CR23">23</xref>]. However, these, in the absence of statutory requirements, are still not enough to prevent the unintentional development of untrustworthy and biased AI systems.</p><p id="Par12">Within the UK we have as yet to meaningfully develop AI specific legislation and regulation. Hence, we still see repeated investment in and use of systems that impact negatively on individuals and groups, despite several government ethics advisory boards and procurement guidelines [<xref ref-type="bibr" rid="CR24">24</xref>]. This has resulted in a reliance on other regulation such as the GDPR, which focuses on data protection but does address profiling and automated decision making (outlined later). However, the GDPR itself is still struggling with implementation [<xref ref-type="bibr" rid="CR25">25</xref>] due to the complexity of the guidance and how it specifically addresses the wider issues involving machine learning and AI. A recent analysis of the he UK Information Commissioner Office guidance [<xref ref-type="bibr" rid="CR26">26</xref>] has concluded that it does not, to date, appear robust enough or sufficiently developed to be deployed meaningfully in the AI space [<xref ref-type="bibr" rid="CR27">27</xref>].</p><p id="Par13">As well as the lack of regulation forcing the requirements for Trustworthy AI another key issue is the lack of awareness and training in the current field. The main training pipelines and education routes that an AI developer might take do not have benchmark subject statements even as of 2019 [<xref ref-type="bibr" rid="CR28">28</xref>] which demonstrates a significant gap in addressing data science or AI in Higher Education. There are some areas of good practice, for example the Level 6 Data Science Apprenticeship Standard outlines key knowledge, skills and behaviours addressing ethical development of AI [<xref ref-type="bibr" rid="CR29">29</xref>]. Professional qualifications are now coming online that also offer future adjustments to professional practice [<xref ref-type="bibr" rid="CR30">30</xref>]. However, developers need to be aware of, open to and subject to a demand for such qualifications. This therefore leaves the challenge of trying to educate developers and modellers after the fact in a process that, without statutory legislation or professional requirements, is occurring slower than the rate of technological development.</p><p id="Par14">The result of this myriad of issues is the obvious human cost, that we have outlined thus far, plus the significant financial loss to public funds and reputational damage due to the withdrawal of expensive and harmful AI solutions. Therefore, it is vital that we influence ethical development of AI at an as early stage as possible to prevent such problems from occurring.</p><p id="Par15">One way in which we can encourage and ensure the development and deployment of Trustworthy AI systems is to influence public and charitable funding. Significant funding is awarded for AI projects and such grants are hotly sought after. The importance of public funds and investment for AI was addressed in the pivotal Hall-Presenti Review for the DCMS<xref ref-type="fn" rid="Fn3">3</xref> and BEIS<xref ref-type="fn" rid="Fn4">4</xref> [<xref ref-type="bibr" rid="CR31">31</xref>]. The report stressed the importance of public funds used to invest in major challenge areas (identified by Innovate UK and ESPRC) such as personalised and integrated health care. The report also highlighted key issues such as transparency, explainability, training and diversity. Though not expressly mentioned it would be fair to assume that such funding should be driven towards projects that are ethically designed to produce trustworthy AI solutions.</p><p id="Par18">There are many examples of funding calls having ethical requirements (these are outlined later) and additional monitoring, for example value for money is usually taken into account in the funding process. Hence, adjusting this process should not require a significant change in mind-set and requirements for funders than already exists.</p><p id="Par19">Addressing the funding of AI systems may act as a significant nudge to require applicants to educate themselves in and apply Ethical/Trustworthy AI principles and design frameworks. Hence, in response to these issues we propose that grant funding and public tendering of AI systems should require a Trustworthy AI Statement within the grant proposal or tendering document. The statement would outline the actions planned by applicants to ensure their project and/or product can be deemed trustworthy and benchmarked against the rigorous standards.</p><p id="Par20">We acknowledge that this solution is not enough on its own to address the issue of untrustworthy public sector algorithms and that it should be embedded within a wider AI governance and regulatory framework. However, small changes within the operational ecosystem funding for AI will provide the nudge technique that is needed to start to circumvent the problems outlined throughout this paper. This will be a simple, easy to implement, change to the application procedure by funding bodies that could result in a fundamental and vital change to the future of the AI ecosystem.</p><p id="Par21">Essentially the message would be: &#x0201c;if you want the money, then build trustworthy AI&#x0201d;.</p></sec><sec id="Sec2"><title>Frameworks for ethical governance of AI</title><p id="Par22">Given that AI and particularly AI ethics is a relatively new field in terms of the wider application of both it may be forgiven that many wishing to utilise or fund AI solutions are not conversant in the potential risks and harms that can occur. However, the AI ethics ecosystem is rapidly developing and providing a large of guidelines, applicable frameworks and tools that researchers can now utilise.</p><sec id="Sec3"><title>High level AI principles</title><p id="Par23">Recent years have seen the publishing of a large number of high level AI principles. A report from the Berkman Klein Center for Internet and Society (Harvard University) listed 36 AI principles documents published by organisations such as the United Nations, the OECD, G20, IEEE and EU Commission, though this is certainly not an exhaustive list [<xref ref-type="bibr" rid="CR32">32</xref>]. Such principles offer normative guidance &#x0201c;for ethical, rights-respecting and socially beneficial AI&#x0201d;. The authors completed a mapping of all 36 principles and 8 central themes:<list list-type="bullet"><list-item><p id="Par24">Privacy.</p></list-item><list-item><p id="Par25">Accountability.</p></list-item><list-item><p id="Par26">Safety and security.</p></list-item><list-item><p id="Par27">Transparency and explainability.</p></list-item><list-item><p id="Par28">Fairness and non-discrimination.</p></list-item><list-item><p id="Par29">Human control of technology.</p></list-item><list-item><p id="Par30">Professional responsibility.</p></list-item><list-item><p id="Par31">Promotion of human values.</p></list-item></list></p><p id="Par32">One of the examples included by the above report is the Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment [<xref ref-type="bibr" rid="CR33">33</xref>]. This was developed by the High-Level Expert Group on Artificial Intelligence (AI HLEG) of the European Commission. The assessment list consists of 7 main categories, each containing subsections as illustrated below<xref ref-type="fn" rid="Fn5">5</xref> (Table <xref rid="Tab1" ref-type="table">1</xref>):</p><p id="Par34">ALTAI provides a checklist of action points for each section. For example, the Accuracy section consists of 5 checkpoints, two of which are listed below:<list list-type="bullet"><list-item><p id="Par35">Could a low level of accuracy of the AI system result in critical, adversarial or damaging consequences?</p></list-item><list-item><p id="Par36">Did you put in place measures to ensure that the data (including training data) used to develop the AI system is up-to-date, of high quality, complete and representative of the environment the system will be deployed in?</p></list-item></list></p><p id="Par37">The Data governance section includes questions regarding the production of a data Privacy Impact Assessment, measures to achieve privacy-by-design, data minimisation and:<list list-type="bullet"><list-item><p id="Par38">Did you align the AI system with relevant standards (e.g. ISO, IEEE) or widely adopted protocols for (daily) data management and governance?</p></list-item></list></p><p id="Par39">The Explainability section contains the following bullet points:<list list-type="bullet"><list-item><p id="Par40">Did you explain the decision(s) of the AI system to the users?</p></list-item><list-item><p id="Par41">Do you continuously survey the users if they understand the decision(s) of the AI system?</p></list-item></list></p></sec><sec id="Sec4"><title>IEEE ethically aligned design</title><p id="Par42">The IEEE&#x02019;s Ethically-aligned-design: Prioritizing human wellbeing with autonomous and intelligent systems was created by over 700 global experts [<xref ref-type="bibr" rid="CR34">34</xref>]. It is noteworthy in that it includes the chapter &#x0201c;Methods to Guide Ethical Research and Design&#x0201d; with subsections on &#x0201c;Interdisciplinary Education and Research&#x0201d;; &#x0201c;Corporate Practices on A/IS&#x0201d; and &#x0201c;Responsibility and Assessment&#x0201d;. The chapter contains a number of recommendations including that ethics training should be a core subject for all those in the STEM field and that relevant accreditation bodies should reinforce an integrated approach to ethics education. The chapter also recommend that:<disp-quote><p id="Par43">&#x02018;corporations should identify stages in their processes in which ethical considerations, &#x0201c;ethics filters&#x0201d;, are in place before products are further developed and deployed&#x02019;.</p></disp-quote></p><p id="Par44">It outlines the example of how ethics review boards would &#x02018;help mitigate the likelihood of creating ethically problematic designs&#x02019;. The document emphasises the importance of stakeholder involvement in design, the importance of algorithmic transparency and recommend the use of human rights and algorithmic impact assessments. This chapter concludes with the recommendation that full documentation should accompany the final product that addresses auditability, accessibility, meaningfulness and readability and that systems are auditable.</p></sec><sec id="Sec5"><title>NHS code of conduct for data-driven health and care technologies</title><p id="Par45">Given that we have looked in more detail at health-related algorithms it is worth noting that the NHS has a code of conduct for data driven technologies [<xref ref-type="bibr" rid="CR35">35</xref>]. The code consists of 12 sections: (1) how to operate ethically, (2) have a clear value proposition, (3) usability and accessibility, (4) technical assurance, (5) clinical safety, (6) data protection, (7) data transparency, (8) cybersecurity, (9) regulation, (10) interoperability and open standards, (11) generate evidence that the product achieves clinical, social, economic or behavioural benefits and (12) define the commercial strategy.</p><p id="Par46">Principle 1 states that:<disp-quote><p id="Par47">&#x02018;Increasing use of data-driven technologies, including artificial intelligence could cause unintended harm if we do not think about issues such as transparency, accountability, safety, efficacy, explicability, fairness, equity and bias.&#x02019;</p></disp-quote></p><p id="Par48">It references the Data Ethics Framework which informs on the development and adoption of safe, ethical and effective digital and data-driven health and care technologies. It again stresses the over-arching principles of accountability, fairness and transparency and suggests a scoring mechanism for analysing the proposed project. The Data Ethics Framework in turn references the Nuffield Council on Bioethics &#x02018;Ethical principles for data&#x02019; highlighting the four principles of:<list list-type="bullet"><list-item><p id="Par49">Respect for persons.</p></list-item><list-item><p id="Par50">Respect for human rights.</p></list-item><list-item><p id="Par51">Participation.</p></list-item><list-item><p id="Par52">Accounting for decisions.</p></list-item></list></p></sec><sec id="Sec6"><title>The GDPR, AI audits and government guidelines</title><p id="Par53">As stated in the introduction the GDPR has not fully addressed the wider aspects of algorithmic systems, with its understandable focus on data privacy. However, the issues of automated decision making and profiling are mentioned and a number of articles and recitals do have applicability in terms of automated-decisions making (ADM) systems, including profiling. Relevant Articles that reference algorithmic systems directly are Articles 13, 14, 15 and 22. These Articles confer the right to be notified if subject to ADM &#x02018;meaningful information about the logic involved&#x02019; including the significance and the envisaged consequences.</p><p id="Par54">Article 22 is focussed on ADM and confers the right to &#x0201c;not be subject to a decision based solely on automated processing, including profiling, which produces legal effects&#x0201d;. As outlined in Article 22 other Articles, which do not explicitly reference ADM also have relevance, such as Article 9 Processing of Special Characteristics. Recital 71 provides further guidance on implementation of Article 22 and references key issues such as fairness, minimisation of errors, transparency and the right to obtain human intervention commonly referred to as human-in-the-loop). Article 35 references the requirement for a Data Privacy Impact Assessments (DPIAs). It has been suggested that the DPIA could be extended to a fuller Algorithmic Impact assessment to allow for a wider assessment of algorithmic risks [<xref ref-type="bibr" rid="CR36">36</xref>]. Article 35 addresses the importance that the system should be reviewed to assess processing remains in accordance with the DPIA. This is important in considering the ethical design of research projects and their further deployment.</p><p id="Par55">AI Auditing is a developing field that has yet to reach maturity but it is highly likely we will see this become a standard requirement in the future [<xref ref-type="bibr" rid="CR37">37</xref>]. An AI Audit must be carried out in a rigorous and robust way to ensure fit for purpose models are deployed. The UK&#x02019;s ICO has published an AI Audit framework [<xref ref-type="bibr" rid="CR38">38</xref>] that addresses, alongside AI-specific risk areas, the governance and accountability of AI systems including audit trails, training and awareness and compliance. For Humanity have devised the Independent Audit of AI systems, a framework for auditing AI systems (products, services and corporations) by examining the downside risks focussing on Privacy, Bias, Ethics, Trust and Cybersecurity [<xref ref-type="bibr" rid="CR39">39</xref>].</p><p id="Par56">Data quality is a key aspect of building an effect model, whether AI based or not. The old adage states &#x0201c;rubbish in equals rubbish out&#x0201d;. Data is the bedrock of any model and quality of this data is of paramount concern. The AQuA Book for best practise within the UK Government states &#x0201c;However, if analysis and any supporting models, data and assumptions are not fit for purpose then the consequences can be severe ranging from financial loss through to reputational damage and legal challenge. In the most severe of consequences, lives and livelihoods can be affected.&#x0201d; [<xref ref-type="bibr" rid="CR40">40</xref>]. In the Lords Enquiry of June 2020 concerns were raised over data availability and quality which went on to impact the models in this area. [<xref ref-type="bibr" rid="CR55">55</xref>]. The Decision Makers Playbook advises scrutiny of data before any type of decision is made and indeed advises Decision Makers to funny understand their data and seek further advice on the data should they need it [<xref ref-type="bibr" rid="CR56">56</xref>]. The European Statistical System Committee. (2012). Quality Assurance Framework of the European Statistical System also discusses how to ensure data quality [<xref ref-type="bibr" rid="CR58">58</xref>] along with the Quality Assurance of Administrative Data (QAAD) framework. These frameworks are not recent and so stand testament to the existing emphasis on data quality.</p><p id="Par57">Guidance and frameworks can also be derived from related areas for example in the UK government guidance on producing quality analysis for government, the AQuA book [<xref ref-type="bibr" rid="CR40">40</xref>], and governmental recommendations for Business Critical models. It is expected that a model would have a suite of robust supporting paperwork, the burden being higher for business critical models. This guidance was developed post the 2013 MacPherson Review [<xref ref-type="bibr" rid="CR57">57</xref>] to ensure fit for purpose modelling and could easily be updated and developed for AI, particularly for those in high risk scenarios such as healthcare, along with recommendations from ethics committees and, in particular, the House of Lords Committee on AI. Indeed, as stated earlier, frameworks for best practise data quality also exist and so, if already adhered to, should not provide further unnecessary burden [<xref ref-type="bibr" rid="CR58">58</xref>]. Legislation for AI systems and mandated regulation, beyond that of data privacy and protection, is also beginning to occur. Most are addressing certain types of AI systems (such as Lethal Autonomous Weapons or self-driving cars) though wider more general AI laws are beginning to be enacted in other countries [<xref ref-type="bibr" rid="CR41">41</xref>].</p></sec><sec id="Sec7"><title>Summary</title><p id="Par58">It is evident that there is a significant body of literature to rely on to enable grant applicants to design proposals incorporating ethics-by-design in the same way as the more established protocols of privacy-by-design [<xref ref-type="bibr" rid="CR42">42</xref>] and security-by-design [<xref ref-type="bibr" rid="CR43">43</xref>]. Yet, despite the amount of guidance it is still evident that there is a challenge in putting these frameworks and principles into practise, resulting in the many problems outlined in the introduction.</p><p id="Par59">As there is a growing impetus in the governance and regulation of AI systems, including audits, there will be increasing accountability and indeed, liability for these system. This further increases the urgency to develop a future-proof system of funding and supporting Trustworthy AI projects being developed now to avoid the harms and waste of funds that can occur.</p></sec></sec><sec id="Sec8"><title>The role of funders in promoting and ensuring trustworthy AI</title><p id="Par60">There are many examples of AI systems being funded, developed and deployed that are not fit for purpose, unethical, unfair, unsafe and further embedding discrimination in society. A key aspect of ethical AI is that of accountability and often the question is raised as to who should bear ultimate accountability and could potentially be held liable. It may be reasonable to predict that at some point, given the increasing number of challenged AI systems, that questions will be asked of those funding questionable projects. This is particularly relevant to funding using public funds and requirement to adhere to certain public standards.</p><p id="Par61">This is illustrated, for example, by the UK Government Committee on Standards in Public Life report on Artificial Intelligence and Public Standards [<xref ref-type="bibr" rid="CR44">44</xref>] which stated that:<disp-quote><p id="Par62">&#x0201c;Explanations for decisions made by machine learning are important for public accountability. Explainable AI is a realistic and attainable goal for the public sector &#x02013; so long as public sector organisations and private companies prioritise public standards when they are designing and building AI systems&#x0201d;</p></disp-quote></p><p id="Par63">And:<disp-quote><p id="Par64">&#x0201c;By ensuring that AI is subject to appropriate safeguards and regulations, the public can have confidence that new technologies will be used in a way that upholds the Seven Principles of Public Life<xref ref-type="fn" rid="Fn6">6</xref>&#x0201d;.</p></disp-quote></p><p id="Par66">Thus, there is a strong moral argument in that it is simply the right thing to ensure the funding of trustworthy AI. As stated earlier grant funding often requires monitoring to ensure value for money and that funded proposals deliver the promised project. Funders therefore have a duty to ensure that AI projects are not untrustworthy thus causing harms and subject to litigation and withdrawal to prevent reputational damage and financial waste.</p><p id="Par67">The UK Parliament Select Committee on Artificial Intelligence has stated in its 2020 report &#x0201c;AI in the UK: No Room for Complacency&#x0201d; [<xref ref-type="bibr" rid="CR45">45</xref>] that:<disp-quote><p id="Par68">&#x0201c;There is a clear consensus that ethical AI is the only sustainable way forward. Now is the time to move the conversation from what are the ethics, to how to instil them in the development and deployment of AI systems&#x0201d;</p></disp-quote></p><p id="Par69">Hence it is now vital that across the AI lifecycle we need practical operational nudges that will simultaneously educate the AI community<xref ref-type="fn" rid="Fn7">7</xref> and practically and immediately promote the development of trustworthy AI. One of the most influential ways to achieve this will be to address how AI is funded and how funding bodies manage the process.</p><p id="Par71">In the coming sections we outline practical suggestions that would enable funding bodies to manage the implementation of ethical funding of AI.</p><sec id="Sec9"><title>How operational &#x02018;nudges&#x02019; can have a wide impact?</title><p id="Par72">One of the solutions we propose in this paper is a simple adjustment to the application procedure which requires a Trustworthy AI Statement, in which applicants must outline their plans to ensure they follow an ethically aligned design approach. Whilst it could reasonably be questioned as to whether such a seemingly small alteration would genuinely cause a difference, there is evidence that small operational changes can have a great effect.</p><p id="Par73">Tackling the lack of diversity in the workplace is a good example to illustrate how smaller &#x02018;nudges&#x02019; can have an impact particularly as the lack of diversity within the technology industry is often referred to root cause for why biased systems are developed and deployed before faults are spotted. This is an area despite many decades of initiatives and investment technology still lags significantly behind other sectors.</p><p id="Par74">Given that many of the problems with AI systems can be traced to unconscious bias at all stages of the AI lifecycle it would therefore seem reasonable to suggest that developers undergo unconscious bias or implicit bias training. However, it is now known that unconscious bias training, or implicit bias training does not reduce bias, alter behaviour or change the workplace [<xref ref-type="bibr" rid="CR46">46</xref>].</p><p id="Par75">Iris Bohnet in her book &#x02018;What works: gender equality by design&#x02019; [<xref ref-type="bibr" rid="CR47">47</xref>] explains that it is easier to change procedures than people and that over time perceptions and opinions will begin to evolve and accept new ways of working. The book highlights the ineffectiveness of unconscious bias training and also how other diversity solutions such as programmes to &#x02018;encourage women&#x02019; can actually often just place an extra focus and work burden on women. Such programmes avoid the real structural issues that have resulted in their exclusion in the first place.</p><p id="Par76">One well known example of a small procedural change that did have significant effect is that of &#x0201c;blind&#x0201d; auditions for orchestras. The general consensus pre-1970 was that men were better musicians than women and this explained the lack of representation of women in orchestras (only 5% were women). However, conducting auditions where the interviewers listened behind a screen, thus unaware of the gender of the musician, was effective in redressing this misconception and proved more effective than all unconscious bias training and mentoring in increasing the numbers of female musicians employed (to 35%). So, it is worth considering similar operational and system changes that can be introduced in the funding requirements that could begin to address the problems experienced regarding AI development. No one singular change is enough but each small &#x02018;nudge&#x02019; when combined can produce great effect.</p></sec></sec><sec id="Sec10"><title>Case studies</title><p id="Par77">Requiring ethical considerations is not an unusual expectation for grant awarding bodies, particularly those within the life science and medical fields. Indeed, any university-led research requiring human involvement requires ethical approval. Below are presented three case studies detailing approaches and methodologies for Ethics Oversight in terms of other aspects such as gender equality, ethics screening and management and formation of specific bioethics boards. The purpose of listing the case studies below is for general consideration of how similar structures could be adapted and applied to AI ethics grant management.</p><sec id="Sec11"><title>Case study 1: GCRF and Newton Fund Gender Equality Statement</title><p id="Par78">An example of a simple approach to addressing ethical issues is the Gender Equality Statement required by the &#x000a3;1.5 billion Global Challenges Research Fund (GCRF) and the &#x000a3;735 million Newton Fund managed in the UK by BEIS and with a number of delivery partners including the UKRI and British Council. This fund builds research and innovation partnerships in partner countries to support their economic development and social welfare. As part of the application a Gender Equality Statement is required in which:<disp-quote><p id="Par79">&#x0201c;applications must outline how they have taken meaningful yet proportionate consideration as to how the project will contribute to reducing gender inequalities in the Gender Equality Statement section of the application form&#x0201d;.</p></disp-quote></p><p id="Par80">The statement is allocated a 3500-character section on the application form and should be statement should be project specific, include the projects outputs and outcomes, the make-up of the project team and all other stakeholders, and refer to the processes followed throughout the research process. It cannot be a re-statement of the institution's policy. If the question is considered not applicable, then the statement should explain why. Five criteria are listed in the application guidance:<list list-type="bullet"><list-item><p id="Par81">Have measures been put in place to ensure equal and meaningful opportunities for people of different genders to be involved throughout the project? This includes the development of the project, the participants in the research and innovation and the beneficiaries of the research and innovation.</p></list-item><list-item><p id="Par82">The expected impact of the project (benefits and losses) on people of different genders, both throughout the project and beyond.</p></list-item><list-item><p id="Par83">The impact on the relations between people of different genders and people of the same gender. For example, changing roles and responsibilities in households, society, economy, politics, power, etc.</p></list-item><list-item><p id="Par84">How any risks and unintended negative consequences on gender equality will be avoided or mitigated against, and monitored.</p></list-item><list-item><p id="Par85">Whether any relevant outcomes and outputs are being measured, with data disaggregated by age and gender (where disclosed).</p></list-item></list></p><p id="Par86">Such weight is given to this section of the application that the application can be rejected if the project proposal is determined to have a negative impact on gender equality or if there is insufficient consideration given within the statement.</p><p id="Par87">As this grant is a broad-based grant it also has a section for applicants to outline the Research Governance and Ethics for the project. This section is subdivided into 3 parts. Firstly, they require an outline of how applicants will ensure the activity will be carried out to the highest standards of ethics and research integrity (2000-characters). Secondly, applicants are requested to outline the potential ethical, health and safety issues (2000-characters). Finally, a sub-section asks if any of the proposed research involves human participation, human tissue, patient/participant data, animal research, genetic and biological risk, arms/military research (including dual-use technologies). If the project does involve any of these aspects, then applicants are required to confirm they have obtained the necessary permission certificates.</p></sec><sec id="Sec12"><title>Case study 2: Horizon 2020 ethical checklist</title><p id="Par88">Horizon 2020 is a 79-billion-euro research and innovation fund running from 2014 to 2020 [<xref ref-type="bibr" rid="CR48">48</xref>]. Its aims are &#x0201c;to ensure that Europe produces world-class science&#x0201d;, &#x0201c;remove barriers to innovation&#x0201d; and &#x0201c;make it easier for public and private sectors to innovate together&#x0201d; to achieve global competitiveness, and to facilitate collaborative innovation so that new projects get off the ground quickly.</p><p id="Par89">Ethics is viewed as an integral part of research from the initial conceptual stage to the finish. As such the Ethics Appraisal Procedure has been used to provide a framework for assessing and conducting an ethically-aligned project, compliant with fundamental ethical principles. It includes an Ethics Review Procedure (which involves ethics screening and assessment) to be conducted before the project start and an Ethics Check and Audit during the implementation phase, summarised in the table below [<xref ref-type="bibr" rid="CR49">49</xref>] (Table <xref rid="Tab2" ref-type="table">2</xref>).</p><p id="Par90">Applicants are required to complete an Ethics Self-Assessment by completion of an Ethics Issues table<xref ref-type="fn" rid="Fn8">8</xref> [<xref ref-type="bibr" rid="CR50">50</xref>]. This is essentially a checklist of actions and list of required documentation. For example in Sect.&#x000a0;4 Protection of Personal Data there is a section addressing profiling:<disp-quote><p id="Par92">&#x0201c;- Does it involve profiling, systematic monitoring of individuals or processing of large scale of special categories of data, intrusive methods of data processing (such as, tracking, surveillance, audio and video recording, geolocation tracking etc.) or any other data processing operation that may result in high risk to the rights and freedoms of the research participants?&#x0201d;</p></disp-quote></p><p id="Par93">Information that is requested for this is:<disp-quote><p id="Par94">&#x0201c;1) Details of the methods used for tracking, surveillance or observation of participants.</p><p id="Par95">2) Details of the methods used for profiling.</p><p id="Par96">3) Risk assessment for the data processing activities.</p><p id="Par97">4) How will harm be prevented and the rights of the research participants safeguarded? Explain.</p><p id="Par98">5) Details on the procedures for informing the research participants about profiling, and its possible consequences and the protection measures.&#x0201d;</p></disp-quote></p><p id="Par99">and documentation requires:<disp-quote><p id="Par100">&#x0201c;1) Opinion of the data controller on the need for a data protection impact assessment (art.35 GDPR) (if relevant).&#x0201d;</p></disp-quote></p><p id="Par101">In the further guidance the checklist states:<disp-quote><p id="Par102">&#x0201c;Personal data must be processed in accordance with certain principles and conditions that aim to limit the negative impact on the persons concerned and ensure <bold>fairness</bold>, <bold>transparency</bold> and <bold>accountability</bold> of the data processing, data quality and confidentiality&#x0201d;.</p></disp-quote></p><p id="Par103">Once the proposal has been submitted and considered for funding the proposal undergoes an Ethics Review. This consists of two phases, an initial Ethics Screening and then, if deemed needed after screening, an Ethics Assessment. This process involves independent ethics experts and qualified staff. The Ethics Review can result in ethics requirements being set as contractual obligations.</p><p id="Par104">As an outcome of the Ethics Review a number of Ethics Requirements and an Ethics Work Package is produced. There are two types of Requirement, those for the grant preparation and then for the ongoing project. The Requirements are included in the grant agreement as project ethics deliverables which are also placed in the work package. If the project breaches the ethics principles an Ethics Audit can occur. Audits can result in changes to the grant agreement and possibly reduction or termination of the grant arrangement.</p></sec><sec id="Sec13"><title>Case study 3: MRC Ethics boards</title><p id="Par105">The Medical Research Council (MRC) has a wide range of resources and guidance for researchers in response. One such development, in response to the advances in medicine and biology, is the Nuffield Council on Bioethics, founded in 1991. The purpose of the board is to act as an independent body that will &#x0201c;identify, examine and report on the ethical questions raised by the advances in biological and medical research&#x0201d; [<xref ref-type="bibr" rid="CR51">51</xref>].</p><p id="Par106">Board funding is currently provided by The Nuffield Foundation, the MRC and the Wellcome Trust. Membership consists of experts from a wide variety of specialisms, including lawyers, educators and philosophers as well as clinical practitioners, leading to a truly multi-disciplinary approach to ethics. The committee provides support with policymaking and addressing public concerns.</p><p id="Par107">In addition, the Ethics, Regulation and Public Involvement Committee (ERPIC) [<xref ref-type="bibr" rid="CR52">52</xref>] also provides high level ethical oversight and guidance. ERPIC is a council of seven experts who advise on policy relating to a wide range of issues including ethics, legislation and regulation and matters relating to research involving animals or human participation (including personal information).</p></sec><sec id="Sec14"><title>Summary</title><p id="Par108">These examples demonstrate that it is possible to insert into an application an additional requirement for applicants to consider a particular ethical aspect. The onus is placed on the applicant to perform the research and find the expertise required to be informed of the specific ethical or governance requirements, such as gender equality issues in case study 1. Case studies 2 and 3 outline a variety of management options where ethical compliance and advice can be provided by a range of qualified staff, experts and ethics boards. Transferring a similar approach to AI ethics would be feasible to do in a fashion similar to the example given.</p></sec></sec><sec id="Sec15"><title>Proposals</title><p id="Par109">These proposals were developed in response to the repeated reporting of AI systems that were found to be discriminatory or found to not meet the general claims for the system. The lead author gathered a team of relevant experts to act as co-authors of the proposals. The team includes a researcher with experience of grant-writing and grant review panels, industry standards and AI ethics experts and representatives of a grant funding body.</p><p id="Par110">The proposals primarily address competitive academic funding calls but it should be noted that such calls often are responded to by joint academic, public sector and/or industry teams that have an intended route to impact. Therefore, we recommend that all R&#x00026;D funding calls relating to AI is subject to a specific trustworthiness assessment. This would take the form of describing how the proposal will address a stated ethical AI framework (either as required by the funding body or chosen by the applicants if not previously specified) and including details of their intended methods. This is outlined by proposal 1 by the insertion of a trustworthy AI statement section in application forms. It should be noted that although there are examples of ethics requirements by funding bodies that cover aspects of AI ethics principles, such as diversity and inclusion and data privacy, none are AI specific and address issues such as explainability of the entire AI system or fairness testing.</p><p id="Par111">It should be stated that these proposals do assume that funders have a degree of accountability in terms of how they choose to direct funds, particularly if this is public money. Additionally, they also have the responsibility to provide the guidance and support to researchers to enable them to respond accordingly. This could be implemented by a simple change to current application forms and guidance (proposal 1), the presence of an AI ethicist on review panels, and/or the constitution of Ethical AI Boards, that could nudge the AI field effectively towards ethical development. Hence, proposal 2 sets out suggestions for funding bodies to consider as to how they would govern this aspect.</p><p id="Par112">Below we outline two key proposal (1) Introduction of a Trustworthy AI Statement and (2) formation of AI Ethics boards.</p><sec id="Sec16"><title>Proposal 1: introduction of a trustworthy AI statement</title><p id="Par113">Hundreds of millions of pounds are offered each year to fund AI projects, not including private investment for start-ups. Grant awarding bodies can step in at a fundamental stage of the most innovative aspects of AI development. Requiring applicants to outline the ethical considerations relevant to their project proposal will provide the opportunity for funding bodies, researchers and developers a point of reflection and the opportunity to identify and mitigate potential problems or harms at the outset.</p><p id="Par114">Criteria can be assessed against existing standards such as aqua/GSS standards/European standards as mentioned previously, e.g., the ALTAI self-assessment toolkit and outlined in accompanying guidance for the grant call. It would be expected that sufficient expertise would be present in the review panel to assess the application.</p><p id="Par115">Here we provide a suggested example of such a requirement and the outline guidance that can be made available.</p></sec><sec id="Sec17"><title>Sample trustworthy AI statement</title><p id="Par116">Applicants are required to consider the potential negative impacts of their proposed system and to mitigate for potential harms.</p><p id="Par117">Applicants must outline in the Trustworthy AI Statement section of the application form how they have taken meaningful action to ensure the AI project aligns with the principles of ethical and responsible AI design.</p><p id="Par118">The consideration and actions should be specific to the project including justification for the research question; the management of data; the make-up of the project team; the identification and make-up of stakeholders, beneficiaries and groups at risk of bias; the outputs, outcomes and processes to be followed throughout the research programme and plans for deployment. It should not be a re-statement of general policies, though these can be referenced with descriptions as to how the policy will be implemented in the context of the proposed project. Diagrams such as risk matrices with mitigations can be included.</p><p id="Par119">The Trustworthy AI Statement must address the following criteria unless a justification is made why a particular criterion is not applicable.</p><p id="Par120">Criteria:<list list-type="bullet"><list-item><p id="Par121">Justify why AI is the right approach to the problem they are trying to address</p></list-item><list-item><p id="Par122">Have measures in place to ensure equal and meaningful opportunities for people from diverse backgrounds, particularly those known to be under-represented such as women, people of colour and people living with a disability, to be involved throughout the project<sub>.</sub></p></list-item><list-item><p id="Par123">The expected impact of the project (benefits and losses) between diverse groups, considering intersectionality; consideration of long term consequences and the approach to managing risks regarding the impact that the technology might have</p></list-item><list-item><p id="Par124"><italic>Data set quality control</italic> Consideration and documentation regarding the provenance of data used, any privacy risks and associated mitigations, data diversity and representativeness and data security measures.</p></list-item><list-item><p id="Par125">Consideration of algorithmic bias and explanations for metrics and fairness tests use plus details of mitigations for any identified bias and future monitoring and whistle-blower protections.</p></list-item><list-item><p id="Par126">Outline how the system will incorporate an Explainable AI approach, avoiding scenarios where the behaviour of a system cannot be explained, after that fact. This is particularly important for projects deemed high impact for example health algorithms. Any use of a system which is not explainable should have a full justification in the context of the rights and freedoms of stakeholders.</p></list-item><list-item><p id="Par127">Transparency statement for the project including commitment to disaggregate data and metrics by gender, age and race.</p></list-item><list-item><p id="Par128">Clear governance around the development and adoption of the technology</p></list-item></list></p><p id="Par129">The &#x0201c;insert name of grant award body&#x0201d; reserves the right to reject the application if no Trustworthy AI statement is made or if the proposal is assessed to result in an unfair outcome on different groups.</p><p id="Par130">Resources to aid applicants can be found here &#x0201c;add link to a website/webpage with list of resources/Ethics checklist&#x0201d;</p></sec><sec id="Sec18"><title>Proposal 2: management and monitoring by funding bodies</title><p id="Par131">The following are proposals for the management and monitoring of the grants. Many such proposals have parallels in other fields, such as bioethics for example, so operational procedures for instituting these proposals have precedence in some cases:</p></sec><sec id="Sec19"><title>Management of grant funding</title><p id="Par132">Each stage of the funding process provides an opportunity to encourage the funding of trustworthy AI. For each funding body, depending on their sector or discipline, different criteria may be necessary to understand the impacts of the proposal. However, funding bodies within sectors may consider adopting a common approach that can be tailored as necessary to facilitate collaboration between funders and reduce the burden for researchers to understand and address the principles of trustworthy AI.</p><p id="Par133">During the funding call:<list list-type="bullet"><list-item><p id="Par134">Funding bodies support applicants by providing resources, or links to resources that will aid the applicant in considering how to design ethical AI.</p></list-item><list-item><p id="Par135">Provide a call specific or general &#x0201c;Ethical AI Checklist&#x0201d; similar to Horizon 2020 Ethics Checklist for project leads to complete, with full cooperation and sponsorship from partner applicants where appropriate.</p></list-item></list></p><p id="Par136">During selection:<list list-type="bullet"><list-item><p id="Par137">Review panels should be diverse (as is current practice).</p></list-item><list-item><p id="Par138">A review panel should have suitable skills at all stages of the grant approval process to assess the trustworthy statement of application. This is particularly important to provide explanations of reasons for rejection based on inadequate ethical AI statements. This potentially could follow the format of PPIE feedback on panels as it cannot be expected all participants are knowledgeable. However, it is recommended that review panels themselves receive some training or guidance on this aspect of the proposal.</p></list-item></list></p><p id="Par139">After funds have been awarded:<list list-type="bullet"><list-item><p id="Par140">Post-funding management of the funded projects should require review of the adherence to the trustworthy statement by grant managers.</p></list-item></list></p></sec><sec id="Sec23"><title>Trustworthy AI boards</title><p><list list-type="bullet"><list-item><p id="Par142">Establish a Trustworthy AI board within (internal) or across funding organisations.</p></list-item><list-item><p id="Par143">The board could take on a role in provide resources (as suggested above) and advising on call specific guidance and checklists.</p></list-item><list-item><p id="Par144">The board could provide an avenue for safe whistleblowing should any person involved in the project have reservations regarding the nature, risks and harms of the projects going forward.</p></list-item><list-item><p id="Par145">Boards could provide expert feedback on proposals during the shortlisting phase with recommendations and feedback or provide representatives to sit on review panels as experts.</p></list-item><list-item><p id="Par146">A trustworthy AI board could play a role in supporting fund managers in ongoing monitoring of projects to ensure they do not diverge from original intent and could assess outcomes.</p></list-item><list-item><p id="Par147">A high level independent board could provide guidance on policy, arising issues and maintain an overview of the guidance offered by the funding organization. A similar structure exists for example within the MRC with the Ethics, regulation and public involvement committee. Other examples of high level boards could be the Nuffield Council on Bioethics. There are pre-existing organisations that could adapt to a similar role or aid the establishment of such a body. Such a group could liaise with the Standards and regulatory organisations.</p></list-item></list></p></sec></sec><sec id="Sec20"><title>Discussion</title><p id="Par148">The overall approach for proposal 1 is &#x02018;Keep It Simple&#x02019; in that it asks for a short section of free text, as opposed to an extensive and complex checklist. This is a core strength as it means that the process is flexible, scalable and, crucially, easy to implement and assess for impact. It aids the applicant in terms of allowing project specific design and inclusion of risk matrices for example and avoids the need for a complicated, integrated approach to ensure that ethical adherence is highlighted in all work streams. Likewise, it also simplifies the review process for the review panel, particularly if specialists are not available. By collating the proposed methods for ensuring the project follows ethical AI principles in one statement it makes it easier for the reviewers to understand and check compliance with their requirements and guidance.</p><p id="Par149">The expectation is that the funding body will provide call specific guidance regarding the expected requirements for ethical design of the proposed AI systems. The funding body may choose to specify an ethical framework that is sector and/or country specific or allow free choice to the applicants to state the framework they intend to work towards. This allows for scaling of the proposal to wider national/international levels. By not specifying a rigid framework we allow for a flexible, adaptable solution that we believe provides a sustainable approach to governance.</p><p id="Par150">Whilst we feel this is a strength of the proposal, and makes the process less onerous for applicants, a limitation may be that funding bodies themselves will require expert support and guidance as to how to design such guidance. This concern led to the development of proposal 2 which includes the utilisation of AI ethics experts as advisors in design of grant calls and on review panels. We also argue that though our proposals maybe seen as a &#x02018;top down&#x02019; approach, particular with regards to proposal 2, there is the opportunity to inform and educate in both directions&#x02014;requiring the grant review bodies and reviewers to understand the process as well as the applicants, who may indeed be the experts. This will therefore influence funders to consider the trustworthy aspects of their calls, to be cognizant of the risks and gather feedback to direct future investment in of AI that has endeavoured to be trustworthy.</p><p id="Par151">Likewise, there is a limitation in terms of applicants who are not fully informed of the principles for &#x02018;ethics-by-design&#x02019; and for organisations ethics panels, as is standard in universities and growing in industry, to also fully understand these principles alongside their RRI (responsible research and innovation) frameworks. We recognise that this maybe a difficult hurdle but it is also a core reason why the proposals have been posited. It is hard to educate people who are unaware of the need or value of adhering to such principles. With regards to university ethics boards this is an area that universities will need to consider in their ethical approval and However, it is normal in developing a grant call to collate a team of experts that can either take responsibility of set work packages e.g. ethical governance and stakeholder engagement, in much the same way health calls have nominated PPIE leads. Likewise, organisation ethics boards can also utilise such expert support, as is currently the practise for example with EDI involvement. Hence, we feel this is not an insurmountable obstacle and that by focussing on the funding stream the proposals offer a driver for faster adoption of ethics-by-design approaches outside of any regulatory requirements that maybe forthcoming.</p><p id="Par152">Should a grant funder opt to implement proposal 1 we anticipate that the short-term implication will be an adjustment in the focus of the proposals put forward to meaningfully attempt to address ethics-by design. It will also alter the discussions and grading of grant proposals and potentially direct funding to projects that aim to be ethical and sustainable. This low cost, easy to implement proposal may indeed be as far as the proposals develop, which is a risk. Another concern is that the proposal could become opaque enough that it risks &#x02018;ethics-washing&#x02019; proposals. However, we do feel that even this alone will at least enable auditable trails to be created as well as points of reflection and that in the longer-term this could become less of an issue as governance and tools are refined.</p><p id="Par153">Additionally, we appreciate that Proposal 1 may initially cause difficulties for grant applicants such as university researchers and collaborating industry partners who are not au-fait with the wider aspects of developing trustworthy AI. This may inhibit them from applying or indeed re-direct their funding applications to calls that do not have this requirement. However, it is important to note that these proposals would exist within a wider developing infrastructure of AI governance, including the strong likelihood of relevant regulation, as well as sector guidelines and the growth of AI standards, auditing and certification. As it is much harder to retro fit an AI system to adhere to ethical design principles the common sense approach would be to build from concept and design correctly first.</p><p id="Par154">In the medium term we believe, as familiarity and acceptance increases, we would see the practice spread and more funding bodies to develop an established infrastructure of expertise and governance to support researchers. We view that proposal 2 will start to be developed at this point in individual funding organisations or even, if there were to be high level buy in, as a singular governing body akin to bioethics boards. We understand that there is a significant cost implication for the development of proposal 2. However, there are in existence several organisations that can offer support and help design frameworks and formation of governance and advisory boards (e.g. in the UK there is the Ada Lovelace Institute, Turing Institute and the Centre for Data Ethics and Innovation). Hence, in the medium term we feel the impact could be that more calls would enact the proposals and the process begins to become an accepted norm and a backbone of ethical support is provided for both funding bodies and researchers.</p><p id="Par155">We anticipate that in sectors that are highly sensitive to the impact of biased algorithms such as medicine that innovative solutions that provide detailed auditable documentation to outline the trustworthiness and planned governance of the system will have a competitive advantage over others. This will then tie in with the array of procurement guidelines for AI that require ethics adherence and so smooth the transition of ethical innovation from concept, through funding to development, procurement and deployment, with ongoing governance. As such, any resistance will need to adjust to respond to market forces and of course to any legislation coming forward.</p><p id="Par156">In the longer term, whilst we accept this is not a singular cure-all we will see more responsible development and, as the flow of money is affected by these proposals, greater investment directed to the ethical design for trustworthy AI projects. Likewise, as the field develops researchers, developers and funding body representatives will become better educated on the requirements for the development of trustworthy AI.</p></sec><sec id="Sec21"><title>Future work</title><p id="Par157">The proposals will be open for review in a stakeholder workshop with participants from funding bodies and academics. Feedback from the stakeholder workshop will be used to modify the proposals and to produce an impact analysis to consider issues raised and proposed mitigations.</p><p id="Par158">Should a funding body choose to introduce the Trustworthy Statement (or a version of it) then it is proposed that the impact should be evaluated. Future research would entail a meta-analysis of the funding of AI projects prior to implementation of the statement and afterwards. This would determine if there were any substantive changes in the types of projects funded, the make-up of the research teams and styles of application. It would also pick out which features of the guidance are enacted, and which do not gain traction. As an adjunct to this it would be pertinent to survey current AI grant award panels for their own knowledge of Trustworthy AI requirements, views of current projects and application procedures and opinion ex-ante of the proposal. Post-hoc, it is proposed that applicants would be surveyed as to their own response to the requirement and how it altered, if at all, attitudes, plans and design of projects. Feedback from the funding organization itself would also be elicited in terms of how the ongoing monitoring of funded projects was managed and any issues therein.</p><p id="Par159">The sharing of quantitative and qualitative themes from successful funding applications can help provide use cases to drive international work to standardise methods, metrics and techniques. We encourage standards bodies and funding bodies to create liaisons to build this feedback loop.</p></sec><sec id="Sec22"><title>Conclusion</title><p id="Par160">The purpose of this report is to address the continuing issues that have been seen due to AI being improperly developed and deployed, often leading to harm on individuals and society. We propose that funding bodies incorporate the requirement for trustworthy AI statements in their application procedure. This process will have a two-fold aim of educating funding applicants as to the importance and processes for developing Trustworthy AI systems and hopefully ensuring only those systems that have addressed issues such as bias for example are funded. We have outlined a simple structure and guidance of such a statement that could be modified to suit sector specific requirements.</p><p id="Par161">The overarching outcome would be to have a significant change in the education of researchers in AI Ethics and thus produce research for which risks and harms have been mitigated against. Not only would this prevent harms to individuals, groups and communities caused by poorly developed and biased AI systems but also prevents large amounts of funding to be wasted on projects that eventually need to be withdrawn.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Summary of categories in Assessment List for the Trustworthy Artificial Intelligence (ALTAI) for self-assessment</p></caption><graphic position="anchor" xlink:href="43681_2021_69_Tab1_HTML" id="MO1"/></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Summary of ethics appraisal steps for the horizon 2020 grant fund</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Activity</th><th align="left">Who?</th><th align="left">When?</th><th align="left">How?</th></tr></thead><tbody><tr><td align="left">Ethics self-assessment</td><td align="left">Applicant</td><td align="left">Application phase</td><td align="left">Consideration of ethical issues of the proposal</td></tr><tr><td align="left">Ethics pre-screening/screening</td><td align="left">Ethics experts and/or qualified staff</td><td align="left">Evaluation phase</td><td align="left">Review of application material</td></tr><tr><td align="left">Ethics assessment (for proposals involving hESC or raising serious ethical issues: severe intervention on humans)</td><td align="left">Ethics experts</td><td align="left">Evaluation/grant preparation phase</td><td align="left">Review of application material</td></tr><tr><td align="left">Ethics check/audit</td><td align="left">Ethics experts</td><td align="left">Implementation phase</td><td align="left">Review of project deliverables/interview with applicants</td></tr></tbody></table></table-wrap></p></sec></body><back><fn-group><fn id="Fn1"><label>1</label><p id="Par3">The definition for Artificial Intelligence (AI) is open to debate, with the definition shifting over time as new advancements occur. For the purpose of this paper we follow the EU Commission HLEG in AI definition &#x0201c;Artificial intelligence (AI) refers to systems that display intelligent behaviour by analysing their environment and taking actions &#x02013; with some degree of autonomy &#x02013; to achieve specific goals. AI-based systems can be purely software-based, acting in the virtual world (e.g. voice assistants, image analysis software, search engines, speech and face recognition systems) or AI can be embedded in hardware devices (e.g. advanced robots, autonomous cars, drones or Internet of Things applications).&#x0201d; [<xref ref-type="bibr" rid="CR53">53</xref>]. We include Machine Learning (ML) in this definition. We define ML as the use of computer algorithms to automatically analyse and learn from large datasets to achieve specific outcomes such as predictions and decisions.</p></fn><fn id="Fn2"><label>2</label><p id="Par9">The Ada Lovelace Institute have recommended that the government engage experts to form the Group of Advisors on Technology in Emergencies (GATE), an advisory body that would act in a similar way to SAGE [<xref ref-type="bibr" rid="CR54">54</xref>].</p></fn><fn id="Fn3"><label>3</label><p id="Par16">Department for Digital, Culture, Media and Sport.</p></fn><fn id="Fn4"><label>4</label><p id="Par17">Department for Business, Energy and Industrial Strategy.</p></fn><fn id="Fn5"><label>5</label><p id="Par33">A point to note with this framework is that it requires a Human Rights Impact Assessment at the outset. This is significant as many of the legal actions being levied against some AI systems are doing so under human rights, privacy or equalities legislation.</p></fn><fn id="Fn6"><label>6</label><p id="Par65">Seven Principles of Public Life: honesty, integrity, openness, leadership, selflessness and accountability.</p></fn><fn id="Fn7"><label>7</label><p id="Par70">The AI Community refers to all those involved in conceiving, funding, procuring, developing, deploying and using the system.</p></fn><fn id="Fn8"><label>8</label><p id="Par91">Not all sections of the checklist are relevant for all projects, with some sections addressing embryo and animal research for example.</p></fn><fn><p><bold>Publisher&#x02019;s Note</bold></p><p>Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="funding-information"><title>Funding</title><p>This work has not been funded.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Conflict of interest</title><p id="Par162">The corresponding author verifies on behalf of all authors that there is no conflict of interest (The Health Foundation is exploring these issues as part of its data analytics for better health strategy, which aims to ensure that analytics and data-driven technologies have a positive impact on everyone&#x02019;s health and health care).</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Clement-Jones, I.: The government&#x02019;s approach to algorithmic decision-making is broken: here&#x02019;s how to fix it. The Guardian (2020). <ext-link ext-link-type="uri" xlink:href="https://www.theguardian.com/technology/2018/mar/17/facebook-cambridge-analytica-kogan-data-algor">https://www.theguardian.com/technology/2018/mar/17/facebook-cambridge-analytica-kogan-data-algor</ext-link></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Cadwallader, C.: How Cambridge Analytica turned Facebook &#x02018;likes&#x02019; into a lucrative political tool. The Guardian (2018). <ext-link ext-link-type="uri" xlink:href="https://www.theguardian.com/technology/2018/mar/17/facebook-cambridge-analytica-kogan-data-algorithm">https://www.theguardian.com/technology/2018/mar/17/facebook-cambridge-analytica-kogan-data-algorithm</ext-link></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Anon: Home Office drops &#x02018;racist&#x02019; algorithm from visa decisions. BBC (2020). <ext-link ext-link-type="uri" xlink:href="https://www.bbc.co.uk/news/technology-53650758">https://www.bbc.co.uk/news/technology-53650758</ext-link></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Anon: We won! Home office to stop using racist visa algorithm. Joint Council for the Welfare of Refugees (2020). <ext-link ext-link-type="uri" xlink:href="https://www.jcwi.org.uk/news/we-won-home-office-to-stop-using-racist-visa-algorithm">https://www.jcwi.org.uk/news/we-won-home-office-to-stop-using-racist-visa-algorithm</ext-link></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Liberty: Liberty wins ground-breaking victory against facial recognition tech. Liberty [Online] (2020). <ext-link ext-link-type="uri" xlink:href="https://www.libertyhumanrights.org.uk/issue/liberty-wins-ground-breaking-victory-against-facial-recognition-tech/">https://www.libertyhumanrights.org.uk/issue/liberty-wins-ground-breaking-victory-against-facial-recognition-tech/</ext-link></mixed-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redden</surname><given-names>J</given-names></name><etal/></person-group><article-title>Datafied child welfare services: unpacking politics, economics and power</article-title><source>Policy Studies</source><year>2020</year><volume>41</volume><issue>5</issue><fpage>507</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1080/01442872.2020.1724928</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">West Midlands Police and Crime Commissioner: Ethics Committee Meeting September 2020. West Midlands Police Commissioner (2020). <ext-link ext-link-type="uri" xlink:href="https://www.westmidlands-pcc.gov.uk/archive/ethics-committee-meeting-september-2020/">https://www.westmidlands-pcc.gov.uk/archive/ethics-committee-meeting-september-2020/</ext-link></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Henry, D.: Electrician fired for refusing to use facial scanning system wins $23000. NZ Herald (2019). <ext-link ext-link-type="uri" xlink:href="https://www.nzherald.co.nz/business/electrician-fired-for-refusing-to-use-facial-scanning-system-wins-23000/VCVCND6KZH5JDSEIOBQLOX4B7A/">https://www.nzherald.co.nz/business/electrician-fired-for-refusing-to-use-facial-scanning-system-wins-23000/VCVCND6KZH5JDSEIOBQLOX4B7A/</ext-link></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Peters, J.: IBM will no longer offer, develop, or research facial recognition technology. The Verge (2020). <ext-link ext-link-type="uri" xlink:href="https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysis-software">https://www.theverge.com/2020/6/8/21284683/ibm-no-longer-general-purpose-facial-recognition-analysis-software</ext-link></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Nagendran, M., et al.: Artificial intelligence versus clinicians: systemic review of design, reporting standards, and claims of deep learning studies. BMJ <bold>368</bold>, m689 (2020)</mixed-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Faes</surname><given-names>L</given-names></name><name><surname>Kale</surname><given-names>A</given-names></name><name><surname>Wagner</surname><given-names>S</given-names></name><name><surname>Fu</surname><given-names>D</given-names></name></person-group><article-title>A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis</article-title><source>Lancet Digital Health</source><year>2019</year><volume>1</volume><issue>6</issue><fpage>E271</fpage><lpage>E297</lpage><pub-id pub-id-type="doi">10.1016/S2589-7500(19)30123-2</pub-id><pub-id pub-id-type="pmid">33323251</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Deeks, A.: The judicial demand for explainable artificial intelligence. Columbia Law Rev. 1829&#x02013;1850 (2019)</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Holmes, A.: AI could be the key to ending discrimination in hiring, but experts warn it can be just as biased as humans. Bus. Insider (2019). <ext-link ext-link-type="uri" xlink:href="https://www.businessinsider.com/ai-hiring-tools-biased-as-humans-experts-warn-2019-10?r=US&#x00026;IR=T">https://www.businessinsider.com/ai-hiring-tools-biased-as-humans-experts-warn-2019-10?r=US&#x00026;IR=T</ext-link></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Turiel, J.D., Aste, T.: Peer-to-peer loan acceptance and default prediction with artificial intelligence. Royal Society Publishing (2020). <ext-link ext-link-type="uri" xlink:href="https://royalsocietypublishing.org/">https://royalsocietypublishing.org/</ext-link>. 10.1098/rsos.191649</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Robinson, J., Thorn, P.: Do chatbots have a role to play in suicide prevention? The conversation (2018). <ext-link ext-link-type="uri" xlink:href="https://theconversation.com/do-chatbots-have-a-role-to-play-in-suicide-prevention-105291">https://theconversation.com/do-chatbots-have-a-role-to-play-in-suicide-prevention-105291</ext-link></mixed-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knoppers</surname><given-names>B</given-names></name><name><surname>Thorogood</surname><given-names>A</given-names></name></person-group><article-title>Ethics and big data in health</article-title><source>Curr. Opin. Syst. Biol.</source><year>2017</year><volume>4</volume><fpage>53</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1016/j.coisb.2017.07.001</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obermeyer</surname><given-names>Z</given-names></name><name><surname>Powers</surname><given-names>B</given-names></name><name><surname>Vogeli</surname><given-names>C</given-names></name><name><surname>Mullainathan</surname><given-names>S</given-names></name></person-group><article-title>Dissecting racial bias in an algorithm used to manage the health of populations</article-title><source>Science</source><year>2019</year><volume>336</volume><issue>6464</issue><fpage>447</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1126/science.aax2342</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Clift, A., Coupland, C.E.A.: Living risk prediction algorithm (QCOVID) for risk of hospital admission and mortality from coronavirus 19 in adults: national derivation and validation cohort study. BMJ <bold>371</bold>, m3731 (2020)</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Meaker, M.: Algorithm used to set vaccine priority order missed key vulnerable groups. The Telegraph (2020). <ext-link ext-link-type="uri" xlink:href="https://www.telegraph.co.uk/technology/2021/01/10/algorithm-used-set-vaccine-priority-order-missed-key-vulnerable/">https://www.telegraph.co.uk/technology/2021/01/10/algorithm-used-set-vaccine-priority-order-missed-key-vulnerable/</ext-link></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Anon: Framework to Review Models. National Audit Office (2016). <ext-link ext-link-type="uri" xlink:href="https://www.nao.org.uk/wp-content/uploads/2016/03/11018-002-Framework-to-review-models_External_4DP.pdf">https://www.nao.org.uk/wp-content/uploads/2016/03/11018-002-Framework-to-review-models_External_4DP.pdf</ext-link>. Accessed 11 Mar 2016</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">IEEE P7001 Working Group: IEEE P7001 transparency of autonomous systems. IEEESA (2021). <ext-link ext-link-type="uri" xlink:href="https://sagroups.ieee.org/7001/">https://sagroups.ieee.org/7001/</ext-link> Accessed 20 Jan 2021</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">IEEE SA: Ethics in action in autonomous and intelligent systems. IEEE (2020). <ext-link ext-link-type="uri" xlink:href="https://ethicsinaction.ieee.org/p7000/">https://ethicsinaction.ieee.org/p7000/</ext-link></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">ISO/IEC: ISO/IEC JTC 1/SC 42 Artificial intelligence. BSI (2021). <ext-link ext-link-type="uri" xlink:href="https://www.iso.org/committee/6794475.html">https://www.iso.org/committee/6794475.html</ext-link></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Gardner, A.: Don&#x02019;t write off government algorithms&#x02014;responsible AI can produce real benefits. The Conversation (2020). <ext-link ext-link-type="uri" xlink:href="https://theconversation.com/dont-write-off-government-algorithms-responsible-ai-can-produce-real-benefits-145895">https://theconversation.com/dont-write-off-government-algorithms-responsible-ai-can-produce-real-benefits-145895</ext-link></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Mikkelson, D., Soller, D.H., Strandell-Jansso, M., et al.: Companies must automate and streamline, or the challenge of GDPR compliance will overwhelm them. McKinsey (2019). <ext-link ext-link-type="uri" xlink:href="https://www.mckinsey.com/business-functions/risk/our-insights/gdpr-compliance-after-may-2018-a-continuing-challenge">https://www.mckinsey.com/business-functions/risk/our-insights/gdpr-compliance-after-may-2018-a-continuing-challenge</ext-link></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Anon: Explaining decisions made with AI. Information Commisioners Office (2020). <ext-link ext-link-type="uri" xlink:href="https://ico.org.uk/for-organisations/guide-to-dp/key-data-protection-themes/explaining-decisions-made-with-artificial-intelligence/">https://ico.org.uk/for-organisations/guide-to-dp/key-data-protection-themes/explaining-decisions-made-with-artificial-intelligence/</ext-link>. Accessed 10 Jan 2021</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Kazim, E., Koshiyama, A.: Explaining decisions made with AI: a review of the co-badged guidance by the ICO and the Turing Institute. SSRN (2020). <ext-link ext-link-type="uri" xlink:href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3656269">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3656269</ext-link>. Accessed 26 Aug 2020</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">QAA: Subject benchmark statement: Computing. QAA (2019). <ext-link ext-link-type="uri" xlink:href="https://www.qaa.ac.uk/docs/qaa/subject-benchmark-statements/subject-benchmark-statement-computing.pdf?sfvrsn=ef2c881_10">https://www.qaa.ac.uk/docs/qaa/subject-benchmark-statements/subject-benchmark-statement-computing.pdf?sfvrsn=ef2c881_10</ext-link>. Accessed Oct 2019</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Anon: Data scientist (Integrated Degree). Institute for Apprenticeship and Technical Education (2019). <ext-link ext-link-type="uri" xlink:href="https://www.instituteforapprenticeships.org/apprenticeship-standards/data-scientist-(integrated-degree)-v1-0">https://www.instituteforapprenticeships.org/apprenticeship-standards/data-scientist-(integrated-degree)-v1-0</ext-link>. Accessed 10 Jul 2019</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">CERTNEXUS: Certified Ethical Emerging Technologist Professional Certificate. Coursera (2021). <ext-link ext-link-type="uri" xlink:href="https://www.coursera.org/professional-certificates/certified-ethical-emerging-technologist">https://www.coursera.org/professional-certificates/certified-ethical-emerging-technologist</ext-link>. Accessed 3 Jan 2021</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Hall, H., Presenti, J.: Growing the artifical intelligence industry in the UK. UK Government (2017). <ext-link ext-link-type="uri" xlink:href="https://www.gov.uk/government/publications/growing-the-artificial-intelligence-industry-in-the-uk">https://www.gov.uk/government/publications/growing-the-artificial-intelligence-industry-in-the-uk</ext-link>. Accessed 15 Oct 2017</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Fjeld, J., Nagy, A.: Principled artificial intelligence. Harvard Publishing (2020). <ext-link ext-link-type="uri" xlink:href="https://cyber.harvard.edu/publication/2020/principled-ai">https://cyber.harvard.edu/publication/2020/principled-ai</ext-link>. Accessed 15 Jan 2020</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">AI HLEG: Assessment list for trustworthy artificial intelligence (ALTAI) for self-assessment European Commission (2020). <ext-link ext-link-type="uri" xlink:href="https://ec.europa.eu/digital-single-market/en/news/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment">https://ec.europa.eu/digital-single-market/en/news/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment</ext-link> (2020)</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">IEEE Standards Association: Ethically Aligned Design Version 2. IEEE (2019). <ext-link ext-link-type="uri" xlink:href="https://standards.ieee.org/industry-connections/ec/ead-v1.html">https://standards.ieee.org/industry-connections/ec/ead-v1.html</ext-link></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Department of Health and Social Care: Code of conduct for data driven health and care technology. UK Government (2020). <ext-link ext-link-type="uri" xlink:href="https://www.gov.uk/government/publications/code-of-conduct-for-data-driven-health-and-care-technology/initial-code-of-conduct-for-data-driven-health-and-care-technology">https://www.gov.uk/government/publications/code-of-conduct-for-data-driven-health-and-care-technology/initial-code-of-conduct-for-data-driven-health-and-care-technology</ext-link></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Kaminski, M.E., Gianclaudio, M.: Algorithmic impact assessments under the GDPR: producing multi-layered explanations. Int. Data Privacy Law <bold>ipaa020</bold>, 6 (2020)</mixed-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morley</surname><given-names>J</given-names></name><etal/></person-group><article-title>From What to how: an initial review of publicly available AI ethics tools, methods and research to translate principles into practices</article-title><source>Sci. Eng. Ethics</source><year>2020</year><volume>26</volume><fpage>2141</fpage><lpage>2168</lpage><pub-id pub-id-type="doi">10.1007/s11948-019-00165-5</pub-id><pub-id pub-id-type="pmid">31828533</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Binns, R.: An overview of the auditing framework for artificial intelligence and its core components. Information Commissioner Office (2019). <ext-link ext-link-type="uri" xlink:href="https://ico.org.uk/about-the-ico/news-and-events/ai-blog-an-overview-of-the-auditing-framework-for-artificial-intelligence-and-its-core-components/">https://ico.org.uk/about-the-ico/news-and-events/ai-blog-an-overview-of-the-auditing-framework-for-artificial-intelligence-and-its-core-components/</ext-link>. Accessed 26 Mar 2019</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Carrier, R.: Independent audit of AI systems&#x02014;FAQ. ForHumanity (2019). <ext-link ext-link-type="uri" xlink:href="https://www.forhumanity.center/blog-posts/2019/8/8/independent-audit-of-ai-systems-faq">https://www.forhumanity.center/blog-posts/2019/8/8/independent-audit-of-ai-systems-faq</ext-link>. Accessed 8 Aug 2019</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">HM Treasury: The Aqua Book: guidance on producing quality analysis for government. UK Government (2015). <ext-link ext-link-type="uri" xlink:href="https://www.gov.uk/government/publications/the-aqua-book-guidance-on-producing-quality-analysis-for-government">https://www.gov.uk/government/publications/the-aqua-book-guidance-on-producing-quality-analysis-for-government</ext-link>. Accessed 26 Mar 2015</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Walch, K.: AI laws are coming. Forbes (2020). <ext-link ext-link-type="uri" xlink:href="https://www.forbes.com/sites/cognitiveworld/2020/02/20/ai-laws-are-coming/?sh=34910dda2b48">https://www.forbes.com/sites/cognitiveworld/2020/02/20/ai-laws-are-coming/?sh=34910dda2b48</ext-link>. Accessed 20 Feb 2020</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">van Rest, J., et al.: Designing privacy by design. privacy technologies and policy. Lect. Notes Comput. Sci. <bold>8319</bold>, 55&#x02013;72 (2014)</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Chehrazi, G., Heimbach, I., Hinz, O.: The impact of security by design on the success of open source software. Research Papers, vol. 179 (2016)</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">The Committee of Standards in Public Life: Artificial Intelligence and Public Standards: report. Uk Parliament (2020). <ext-link ext-link-type="uri" xlink:href="https://www.gov.uk/government/publications/artificial-intelligence-and-public-standards-report">https://www.gov.uk/government/publications/artificial-intelligence-and-public-standards-report</ext-link>. Accessed 10 Feb 2020</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">House of Lords Liaison Committee: AI in the UK: No room for complacency. UK Parliament (2020). <ext-link ext-link-type="uri" xlink:href="https://publications.parliament.uk/pa/ld5801/ldselect/ldliaison/196/19602.htm">https://publications.parliament.uk/pa/ld5801/ldselect/ldliaison/196/19602.htm</ext-link>. Accessed 18 Dec 2020</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Dobbin, F., Kalev, A.: Why doesn&#x02019;t diversity training work? Anthropol. Now <bold>10</bold>, 48&#x02013;55 (2018)</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Bohnet, I.: What works: gender equality by design. Belknap Press of Harvard University Press, Cambridge (2016)</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Innovate UK and UK Research and Innovation: Horizon 2020: what it is and how to apply for funding. UK Government (2020). <ext-link ext-link-type="uri" xlink:href="https://www.gov.uk/guidance/horizon-2020-what-it-is-and-how-to-apply-for-funding">https://www.gov.uk/guidance/horizon-2020-what-it-is-and-how-to-apply-for-funding</ext-link>. Accessed 24 Dec 2020</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">European Commission: Ethics: European Commission (2020). <ext-link ext-link-type="uri" xlink:href="https://ec.europa.eu/research/participants/docs/h2020-funding-guide/cross-cutting-issues/ethics_en.htm">https://ec.europa.eu/research/participants/docs/h2020-funding-guide/cross-cutting-issues/ethics_en.htm</ext-link>. Accessed 14 Jan 2021</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Directorate-General for Research and Innovation: Horizon 2020 Programme: How to complete your ethics self-assessment. European Commission (2020). <ext-link ext-link-type="uri" xlink:href="https://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/ethics/h2020_hi_ethics-self-assess_en.pdf">https://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/ethics/h2020_hi_ethics-self-assess_en.pdf</ext-link>. Accessed 4 Feb 2019</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Medical Research Council: The Nuffield council on bioethics. UKRI (2020). <ext-link ext-link-type="uri" xlink:href="https://mrc.ukri.org/research/policies-and-guidance-for-researchers/the-nuffield-council-on-bioethics/">https://mrc.ukri.org/research/policies-and-guidance-for-researchers/the-nuffield-council-on-bioethics/</ext-link></mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Medical Research Council. Ethics, Regulation &#x00026; Public Involvemnet Committee (ERPIC). UKRI (2020). <ext-link ext-link-type="uri" xlink:href="https://mrc.ukri.org/research/policies-and-guidance-for-researchers/erpic/#:~:text=The%20Ethics%2C%20Regulation%20and%20Public,issues%20relating%20to%20medical%20research">https://mrc.ukri.org/research/policies-and-guidance-for-researchers/erpic/#:~:text=The%20Ethics%2C%20Regulation%20and%20Public,issues%20relating%20to%20medical%20research</ext-link></mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">EU Commission: A definition of artificial intelligence: main capabilities and scientific disciplines. Eu Commission (2019) <ext-link ext-link-type="uri" xlink:href="https://ec.europa.eu/digital-single-market/en/news/definition-artificial-intelligence-main-capabilities-and-scientific-disciplines">https://ec.europa.eu/digital-single-market/en/news/definition-artificial-intelligence-main-capabilities-and-scientific-disciplines</ext-link>. Accessed 9 Apr 2019</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Ada Lovelace Institute: COVID-19 rapid evidence review: Exit through the App Store. Ada Lovelace Institute (2020). <ext-link ext-link-type="uri" xlink:href="https://www.adalovelaceinstitute.org/evidence-review/covid-19-rapid-evidence-review-exit-through-the-app-store/">https://www.adalovelaceinstitute.org/evidence-review/covid-19-rapid-evidence-review-exit-through-the-app-store/</ext-link>. Accessed 19 Apr 2020</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Lords Select Committee on Science and Technology: Lords select committee on science and technology afternoon session&#x02014;co-rected oral evidence: The Science of COVID-19 (London, 2 June 2020) (2020). <ext-link ext-link-type="uri" xlink:href="https://committees.parliament.uk/oralevidence/444/pdf/">https://committees.parliament.uk/oralevidence/444/pdf/</ext-link></mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">The Decision Makers Playbook Publisher: Pearson Education Limited (2020) (ISBN: 9781292129334)</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">HM Treasury: Review of quality assurance of government analytical models. (2013). <ext-link ext-link-type="uri" xlink:href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/206946/review_of_qa_of_govt_analytical_models_final_report_040313.pdf">https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/206946/review_of_qa_of_govt_analytical_models_final_report_040313.pdf</ext-link></mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">European Statistical System Committee: Quality assurance framework of the european statistical system (2012)</mixed-citation></ref></ref-list></back></article>